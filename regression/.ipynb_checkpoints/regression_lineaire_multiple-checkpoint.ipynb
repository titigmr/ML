{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Régression régularisée \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour cette partie on utilise le même jeu de données utilisé dans le cas de la régression linéaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "x, y = make_regression(n_samples=100, n_features=1, noise=10)\n",
    "biais = np.ones((x.shape))\n",
    "\n",
    "y = y.reshape((100,1))\n",
    "X = np.hstack((x, biais))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le sur-apprentissage ou Overfiting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'overfiting consiste pour un modèle de bien apprendre sur les données d'apprentissage mais ne pas réussir à bien prédire à l'arrivée de nouvelles données. Ce modèle aura capturer trop de complexités (bruit aléatoire) de l'échantillon d'entraînement et ne sera pas en mesure de généraliser sur de nouvelles données. Plusieurs facteurs engendre l'overfitting : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **La complexité du modèle** : lorsque qu'un modèle possède beaucoup de coefficients, ce dernier a beaucoup plus de chance d'expliquer trop de détails liés au bruit de variabilité. Ceci fait que notre population à un moment données, présente une corrélation hasardeuse qui ne fonctionnerait pas sur un autre échantillon. Au niveau statistique le $t$ Student donnerait un p-value inférieur à 5% alors que cette corrélation n'est que le fruit du hasard. Ceci s'explique par le fait que la distribution des *p-values* est uniformes. Donc à partir de 20 variables il y a une variable corrélée significativement (seuil de 5%) avec notre variable de sortie.\n",
    "- **La multicolinéarité** : certaines variables d’entrés peuvent être liées ce qui engendre de la redondance d'informations et peut conduire à modifier les estimateurs.\n",
    "- **La haute dimensionnalité** : si un modèle possède trop de coefficients, comparé à son nombre d'individus, il aura tendance à apprendre beaucoup sur ce petit échantillon d'individus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression Ridge : pénalisation $L_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression **Ridge** ajoute un paramètre de pénalité (égale à la distance euclidienne des paramètres) afin de forcer les coefficients à être le plus petit possible, sans les réduire à zéros :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Ridge = \\sum(y_i - X_i \\beta)^2 + \\lambda \\vert\\vert \\, \\beta \\, \\vert\\vert_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le calculer :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\beta = (X^TX + \\lambda I)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X, y, penalty):\n",
    "    return np.linalg.inv(\n",
    "        X.T.dot(X) + penalty * np.identity(X.shape[1])\n",
    "    ).dot(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42.57746844],\n",
       "       [-0.81086131]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge(X, y, penalty=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, beta):\n",
    "    n = len(y)\n",
    "    return 1/(2*n) * np.sum((prediction(X, beta) - y)**2)\n",
    "\n",
    "def gradient(X, y, beta):\n",
    "    n = len(y)\n",
    "    return 1/n * X.T.dot(prediction(X, beta) - y)\n",
    "\n",
    "def prediction(X, beta):\n",
    "    return X.dot(beta)\n",
    "\n",
    "def ridge_gradient_descent(X, y, beta, learning_rate, n_iterations, penalty=0.0):\n",
    "\n",
    "    cost_history = []\n",
    "    n = len(y)\n",
    "    for i in range(0, n_iterations):\n",
    "\n",
    "        beta = beta - learning_rate * \\\n",
    "            (gradient(X, y, beta) + (penalty / n * beta))\n",
    "        cost_history.append(loss(X, y, beta))\n",
    "\n",
    "    return beta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = np.random.randn(2, 1)\n",
    "\n",
    "coef, loss_ridge = ridge_gradient_descent(X, y, beta, learning_rate=0.01,\n",
    "                                          n_iterations=1000, penalty=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42.57565418],\n",
       "       [-0.80981402]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut vérifier avec l'API sklearn :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.577516134630706 \n",
      " -0.811675024493526\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "r = Ridge(alpha=0.1)\n",
    "r.fit(X, y)\n",
    "\n",
    "print(f\"{r.coef_[0][0]} \\n {r.intercept_[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Régression Lasso : pénalisation $L_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression Lasso pénalise les coefficients par la norme l1 (distance de manhattan). Cette contrainte a pour conséquence d'introduire du biais. Par cette régularisation, les coefficients les moins significatifs sont réduits vers zéro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_gradient_descent(X, y, beta, learning_rate, n_iterations, penalty=0.0):\n",
    "    cost_history = []\n",
    "    n = len(y)\n",
    "    \n",
    "    for i in range(0, n_iterations):\n",
    "        beta = beta - learning_rate * \\\n",
    "            (gradient(X, y, beta) + (penalty / n * sum(abs(beta))))\n",
    "        \n",
    "        cost_history.append(loss(X, y, beta))\n",
    "\n",
    "    return beta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_coef, lasso_loss = lasso_gradient_descent(\n",
    "    X, y, beta, learning_rate=0.01, n_iterations=1000, penalty=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38.59169807],\n",
       "       [-4.92189438]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_eq(X, y, penalty):\n",
    "    return np.linalg.inv(\n",
    "        X.T.dot(X) + penalty * np.identity(X.shape[1])).dot(X.T.dot(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([32.77523069,  0.        ]), array([-0.22541978]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lass = Lasso(alpha=10).fit(X, y)\n",
    "lass.coef_, lass.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention** : Toutes les hypothèses de test concernant la régression s’écroulent si on ajoute ce terme de pénalités. \n",
    "\n",
    "Cette régularisation s'utilise dans le cas où on veut juste bien prédire (sans contraintes). Si jamais il y avait un problème de **multicolinéarité**, la pénalisation peut régler ce problème d’overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rasoir d’Ockham** : parmi les solutions on veut la plus simple. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = LassoRegression(learning_rate=0.1, iterations=1000, l1_penality=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "k = pd.DataFrame(X[:,0].reshape(len(X), 1))\n",
    "m = pd.DataFrame(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

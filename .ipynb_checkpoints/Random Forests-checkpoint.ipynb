{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les algorithmes **d'arbre de décision** pour de la classification ([disponible ici](https://github.com/titigmr/ML/blob/master/CART.ipynb)) sont des modèles de machine learning non-linéaire. \n",
    "\n",
    "Il est construit sur le principe de « série de questions » (appelées *split*, *noeuds* ou encore *embranchements*). Ces questions conduisent sucessivement à une feuille de l'arbre (c'est à dire quand un split n'est plus possible). Pour y arriver, l'algorithme utilise le concept d'impureté de Gini. Cette métrique indique à quel point un noeud est pur (autrement dit à quel point une seule modalité apparait sur un embranchement). \n",
    "\n",
    "Pour ce faire, l'arbre de décision forme des noeuds en recherchant pour chacune des variables le split binaire optimal (qu'on appelle *thresholds*). L'objectif est de maximiser l'indice de Gini parmi tous les seuils possibles et retenir le meilleur indice (le plus faible) parmi l'ensemble des variables. \n",
    "\n",
    "Pour finir, le modèle s'arrête dès que tous les noeuds sont purs ou lorsqu'il n'est plus possible de séparer un sous-échantillon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toutefois, un des grands désavantage de l'arbre de décision est qu'il est très sensible à l'effet de surapprentissage dès que le nombre d'embranchement augmente (la capaciter de bien généraliser sur de nouvelles données). Ce surapprentissage se produit car le modèle est très flexible (**forte variance**) et capture sur les données tout le bruit présent.\n",
    "\n",
    "De l'autre côté, si nous limitons le nombre d'embranchements, nous limitons certe la variance du modèle (donc le fait que le modèle puisse sur-apprendre) mais en contre-partiele modèle ne sera pas capable de bien prédire car des hypothèses forte sur les données sont avancées.\n",
    "\n",
    "Au lieu de limiter la profondeur de l'arbre, ce qui réduit la variance et augmente le biais, nous pouvons combiner de nombreux arbres de décision en un seul modèle d'ensemble connu sous le nom de forêt aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La Forêt aléatoire ou Random Forests est un modèle dont le but est de combiner de nombreux arbres de décision. Toutefois, ce modèle possède deux spécificité. Plutôt que de simplement faire la moyenne/vote de la prédiction de chacun des arbres présent dans la fôret, ce modèle utilise deux concepts clés qui lui donnent le nom aléatoire :\n",
    "\n",
    "- **Bootstrapping** : Échantillonnage aléatoire des données d'entraînement lors de la création de chaque arbres ; \n",
    "- **Sélection aléatoires des features** : Sous-ensembles aléatoires de variables pris en compte lors de la division des nœuds ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'objectif du bootstraping est d'utiliser comme données d'entraînmenet un échantillon aléatoire du jeu de données. Les échantillons sont sélectionnés avec remise, appelé *bootstrap*, ce qui signifie que certaines observations seront utilisés plusieurs fois. \n",
    "\n",
    "L'idée est qu'en entraînant chaque arbre sur différents échantillons, bien que chaque arbre puisse avoir une variance élevée par rapport à un ensemble particulier de données d'apprentissage, dans l'ensemble, la forêt entière aura une variance plus faible, mais pas au prix d'une augmentation du biais.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(X, y):\n",
    "    indices = np.random.randint(len(y), size=len(y))\n",
    "    return X.loc[indices], y.loc[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_, y_ = bootstrap(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Au moment de la prédiction, celle-ci est faite en faisant : \n",
    "\n",
    "- la moyenne des prédictions de chaque modèle (dans la cas d'une variable quantitative) ;\n",
    "- le vote parmi la prédition majoritaire pour l'ensemble des modèles (dans le cas d'une variable qualitative).\n",
    "\n",
    "Cette méthode d'ensemble qui utilise des bootstrap des données, puis de calcul de la moyenne/vote des prédictions est appelé : **Bagging** (abréviation de Bootstrap AGGregatING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'autre concept principal est qu'un seul sous-ensemble de variables est pris en compte pour diviser chaque nœud dans chaque arbre de décision. Généralement, il est défini comme `sqrt (n_features)` pour de la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(columns, metric='auto'):\n",
    "    \n",
    "    if metric == 'auto':\n",
    "        n = int(np.sqrt(len(columns)))\n",
    "        selected_features = np.random.choice(columns, size=n, replace=False)\n",
    "    \n",
    "    if metric == 'all':\n",
    "        selected_features = columns\n",
    "        \n",
    "    if type(metric) == int:\n",
    "        n = metric\n",
    "        selected_features = np.random.choice(columns, size=n, replace=False)\n",
    "        \n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sepal length (cm)', 'petal length (cm)'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select_features(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction de la fôret : un ensemble d'arbres de décision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à l'implémentation du Random Forest. Dans un premier temps, il faut multiplier les arbre de décision simples [voir algorithme CART](https://github.com/titigmr/ML/blob/master/CART.ipynb). Dans un second temps, lorqu'une prédiction est faite, agréger l'ensemble des prédictions de chacun des arbres et retenir celle qui a le plus de vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CART import DecisionTreeClassif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_forest(X, y, n_estimators=5, target_name=None):\n",
    "\n",
    "    all_tree = []\n",
    "\n",
    "    for i in range(n_estimators):\n",
    "        X_boot, y_boot = bootstrap(X, y)\n",
    "\n",
    "        X_boot.reset_index(drop=True, inplace=True)\n",
    "        y_boot.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        clf = DecisionTreeClassif(max_features='auto')\n",
    "        clf.fit(X_boot, y_boot, target_name=target_name)\n",
    "\n",
    "        all_tree.append(clf)\n",
    "\n",
    "    return all_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prédiction des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La dernière étape consiste à agréger les prédictions de chaque arbre de la forêt pour fournir une meilleur prédiction qu'un simple arbre de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassif:\n",
    "    \n",
    "    \n",
    "    def __init__(self, max_depth=float('inf'), max_features=\"auto\", n_estimators=5):\n",
    "        \n",
    "        self.max_depth = max_depth + 1\n",
    "        self.max_features = max_features\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, target_name=None):\n",
    "        \n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        all_tree = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            X_boot, y_boot = self.__bootstrap__(X, y)\n",
    "\n",
    "            X_boot.reset_index(drop=True, inplace=True)\n",
    "            y_boot.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            clf = DecisionTreeClassif(max_features=self.max_features)\n",
    "            clf.fit(X_boot, y_boot, target_name=target_name)\n",
    "\n",
    "            all_tree.append(clf)\n",
    "\n",
    "        self.__all_tree__ = all_tree\n",
    "    \n",
    "    def __bootstrap__(self, X, y):\n",
    "        indices = np.random.randint(len(y), size=len(y))\n",
    "        return X.loc[indices], y.loc[indices]\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        return [self._predict_(inputs, X) for inputs in X.index]\n",
    "\n",
    "\n",
    "    def _predict_(self, inputs, X):\n",
    "        \n",
    "        X.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        all_predicts = []\n",
    "        for tree in self.__all_tree__ :\n",
    "            node = tree.tree\n",
    "            \n",
    "            while node.node.left:\n",
    "                if X.loc[inputs, node.node.var] < node.node.threshold:\n",
    "                    node = node.node.left\n",
    "\n",
    "                else:\n",
    "                    node = node.node.right\n",
    "\n",
    "                if node.node is None:\n",
    "                    break\n",
    "\n",
    "            all_predicts.append(node.classe_predict)\n",
    "            \n",
    "        return mode(all_predicts).mode[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On divise l'échantillon en train et test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassif(max_depth=4)\n",
    "clf.fit(X_train, y_train, iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(clf.predict(X_test) == iris.target_names[y_test]) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avec sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=4, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdf = RandomForestClassifier(max_depth=4)\n",
    "rdf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(rdf.predict(X_test) == y_test) / len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Références"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Towardsdatascience.com : RandomForest](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
